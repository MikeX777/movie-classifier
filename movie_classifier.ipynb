{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAP 6619-002\n",
    "# Movie Classifier\n",
    "# Dr. Zhu\n",
    "# Michael Cuomo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import collections\n",
    "import pathlib\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import TextVectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10_Things_I_Hate_About_You.txt', '12.txt', '127_Hours.txt', '12_and_Holding.txt', '12_Monkeys.txt', '12_Years_a_Slave.txt', '1492_Conquest_of_Paradise.txt', '15_Minutes.txt', '17_Again.txt', '187.txt', '2001_A_Space_Odyssey.txt', '2012.txt', '28_Days_Later.txt', '30_Minutes_or_Less.txt', '44_Inch_Chest.txt', '48_Hrs.txt', '50-50.txt', '500_Days_of_Summer.txt', '8MM.txt', '9.txt']\n"
     ]
    }
   ],
   "source": [
    "directory = os.fsencode(\".\\\\data\\\\keywords\")\n",
    "movie_names = []\n",
    "for file in os.listdir(directory):\n",
    "    movie_names.append(os.fsdecode(file))\n",
    "\n",
    "print(movie_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder():\n",
    "    VOCAB_SIZE = 50000\n",
    "    encoder = tf.keras.layers.TextVectorization(max_tokens = VOCAB_SIZE)\n",
    "    dataset = tf.data.TextLineDataset(filenames = list(map(lambda x: f\".\\\\data\\\\scripts\\\\{x}\", movie_names)))\n",
    "    for line in dataset.take(5):\n",
    "        print(line.numpy())\n",
    "    encoder.adapt(dataset.batch(1024))\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_encoder(vocab_size = 50000):\n",
    "    if vocab_size != 50000 and vocab_size != 10000 and vocab_size != 5000:\n",
    "        raise ValueError(\"Vocab of that size does not exist\")\n",
    "    encoder = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
    "    f = open(f\".\\\\data\\\\vocabs\\\\vocab_{vocab_size}.txt\", \"r\")\n",
    "    vocab = f.read()\n",
    "    f.close()\n",
    "\n",
    "    encoder.set_vocabulary(vocab.split(\",\"))\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10_Things_I_Hate_About_You.txt', '12.txt', '127_Hours.txt', '12_and_Holding.txt', '12_Monkeys.txt', '12_Years_a_Slave.txt', '1492_Conquest_of_Paradise.txt', '15_Minutes.txt', '17_Again.txt', '187.txt', '2001_A_Space_Odyssey.txt', '2012.txt', '28_Days_Later.txt', '30_Minutes_or_Less.txt', '44_Inch_Chest.txt', '48_Hrs.txt', '50-50.txt', '500_Days_of_Summer.txt', '8MM.txt', '9.txt']\n"
     ]
    }
   ],
   "source": [
    "directory = os.fsencode(\".\\\\data\\\\keywords\")\n",
    "movie_names = []\n",
    "for file in os.listdir(directory):\n",
    "    movie_names.append(os.fsdecode(file))\n",
    "\n",
    "print(movie_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in movie_names:\n",
    "    with open(f\".\\\\data\\\\scripts_old\\\\{movie}\", 'r') as r, open(f'.\\\\data\\\\scripts\\\\{movie}', 'w', encoding = 'utf8') as o: \n",
    "        for line in r: \n",
    "            if line.strip(): \n",
    "                o.write(line.strip() +'\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSuccinctOutputVectorModel(encoder = get_existing_encoder(50000)):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(encoder)\n",
    "    model.add(tf.keras.layers.Embedding(input_dim = len(encoder.get_vocabulary()), output_dim = 1024, mask_zero = True))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024)))\n",
    "    model.add(tf.keras.layers.Dense(24000, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(17466))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def succintGenerator(seed = 42, train_split = 0.75, is_train = True):\n",
    "    for movie in movie_names:\n",
    "        s = open(f\".\\\\data\\\\scripts\\\\{movie}\", \"r\")\n",
    "        script_lines = random.Random(seed).shuffle(s.read().split(\"\\n\"))\n",
    "        s.close()\n",
    "        if is_train:\n",
    "            script_lines = script_lines[:int(len(script_lines) * train_split) + 1]\n",
    "        else:\n",
    "            script_lines = script_lines[-(int(len(script_lines) * train_split) + 1):]\n",
    "        k = open(f\".\\\\data\\\\vectorized_keywords_succinct\\\\{movie}\", \"r\")\n",
    "        keywords = k.read().split(\",\")\n",
    "        k.close()\n",
    "        for line in script_lines:\n",
    "            yield line, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSuccintGenerator():\n",
    "    yield succintGenerator(seed = 42, train_split = 0.75, is_train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testSuccintGenerator():\n",
    "    yield succintGenerator(seed = 42, train_split = 0.75, is_train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSuccinctTrainDataset():\n",
    "    dataset = tf.data.Dataset.from_generator(trainSuccintGenerator, output_signature = (\n",
    "        tf.TensorSpec(shape = (), dtype = tf.string),\n",
    "        tf.TensorSpec(shape = (17466,), dtype = tf.int8)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSuccinctTestDataset():\n",
    "    dataset = tf.data.Dataset.from_generator(testSuccintGenerator, output_signature = (\n",
    "        tf.TensorSpec(shape = (), dtype = tf.string),\n",
    "        tf.TensorSpec(shape = (17466,), dtype = tf.int8)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_map(data, labels):\n",
    "    return data, tf.stack(labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createSuccinctOutputVectorModel(get_existing_encoder(50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = getSuccinctTrainDataset()\n",
    "test = getSuccinctTestDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parse_map() missing 1 required positional argument: 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\school\\CAP6619\\research\\movie_classifier.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/school/CAP6619/research/movie_classifier.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(parse_map(train), epochs\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/school/CAP6619/research/movie_classifier.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                     validation_data\u001b[39m=\u001b[39mtest,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/school/CAP6619/research/movie_classifier.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                     validation_steps\u001b[39m=\u001b[39m\u001b[39m3000\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: parse_map() missing 1 required positional argument: 'labels'"
     ]
    }
   ],
   "source": [
    "history = model.fit(train.map(pa), epochs=1000,\n",
    "                    validation_data=test,\n",
    "                    validation_steps=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f\".\\\\data\\\\vocabs\\\\vocab_50000.txt\", \"w\")\n",
    "f.write(str(encoder.get_vocabulary()))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_10000 = encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_5000 = encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(encoder.get_vocabulary()))\n",
    "print(len(encoder_5000.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'protective-father'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\school\\CAP6619\\research\\movie_classifier.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/school/CAP6619/research/movie_classifier.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         keyword_list\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mnot-a-keyword\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/school/CAP6619/research/movie_classifier.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     y_labels\u001b[39m.\u001b[39mappend(keyword_list)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/school/CAP6619/research/movie_classifier.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m y_labels \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mto_categorical(y_labels, dtype \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/school/CAP6619/research/movie_classifier.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(y_labels)\n",
      "File \u001b[1;32mc:\\Users\\mcuom\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\np_utils.py:62\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkeras.utils.to_categorical\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_categorical\u001b[39m(y, num_classes\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     25\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Converts a class vector (integers) to binary class matrix.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[39m    E.g. for use with `categorical_crossentropy`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39m    [0. 0. 0. 0.]\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(y, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mint\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m     input_shape \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mshape\n\u001b[0;32m     65\u001b[0m     \u001b[39m# Shrink the last dimension if the shape is (..., 1).\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'protective-father'"
     ]
    }
   ],
   "source": [
    "y_labels = []\n",
    "\n",
    "keywords_directory = os.fsencode(\".\\\\data\\\\keywords\")\n",
    "for file in os.listdir(keywords_directory):\n",
    "    f = open(f\".\\\\data\\\\keywords\\\\{os.fsdecode(file)}\", \"r\")\n",
    "    keywords = f.read()\n",
    "    f.close()\n",
    "    keyword_list = keywords.split(\",\")\n",
    "    while len(keyword_list) < 50:\n",
    "        keyword_list.append(\"not-a-keyword\")\n",
    "    y_labels.append(keyword_list)\n",
    "\n",
    "y_labels = tf.keras.utils.to_categorical(y_labels, dtype = \"string\")\n",
    "print(y_labels)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
